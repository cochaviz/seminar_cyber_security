@misc{abadSecurityPrivacyFederated2022,
  title = {On the {{Security}} \& {{Privacy}} in {{Federated Learning}}},
  author = {Abad, Gorka and Picek, Stjepan and {Ram{\'i}rez-Dur{\'a}n}, V{\'i}ctor Julio and Urbieta, Aitor},
  year = {2022},
  month = mar,
  number = {arXiv:2112.05423},
  eprint = {2112.05423},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-05-25},
  abstract = {Recent privacy awareness initiatives such as the EU General Data Protection Regulation subdued Machine Learning (ML) to privacy and security assessments. Federated Learning (FL) grants a privacy-driven, decentralized training scheme that improves ML models' security. The industry's fast-growing adaptation and security evaluations of FL technology exposed various vulnerabilities that threaten FL's confidentiality, integrity, or availability (CIA). This work assesses the CIA of FL by reviewing the state-of-the-art (SoTA) and creating a threat model that embraces the attack's surface, adversarial actors, capabilities, and goals. We propose the first unifying taxonomy for attacks and defenses and provide promising future research directions.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Cryptography and Security},
  file = {/home/zohar/Zotero/storage/DZI5SDPU/Abad et al. - 2022 - On the Security & Privacy in Federated Learning.pdf}
}

@misc{carliniExtractingTrainingData2021,
  title = {Extracting {{Training Data}} from {{Large Language Models}}},
  author = {Carlini, Nicholas and Tramer, Florian and Wallace, Eric and Jagielski, Matthew and {Herbert-Voss}, Ariel and Lee, Katherine and Roberts, Adam and Brown, Tom and Song, Dawn and Erlingsson, Ulfar and Oprea, Alina and Raffel, Colin},
  year = {2021},
  month = jun,
  number = {arXiv:2012.07805},
  eprint = {2012.07805},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2012.07805},
  urldate = {2023-05-25},
  abstract = {It has become common to publish large (billion parameter) language models that have been trained on private datasets. This paper demonstrates that in such settings, an adversary can perform a training data extraction attack to recover individual training examples by querying the language model. We demonstrate our attack on GPT-2, a language model trained on scrapes of the public Internet, and are able to extract hundreds of verbatim text sequences from the model's training data. These extracted examples include (public) personally identifiable information (names, phone numbers, and email addresses), IRC conversations, code, and 128-bit UUIDs. Our attack is possible even though each of the above sequences are included in just one document in the training data. We comprehensively evaluate our extraction attack to understand the factors that contribute to its success. Worryingly, we find that larger models are more vulnerable than smaller models. We conclude by drawing lessons and discussing possible safeguards for training large language models.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Cryptography and Security,Computer Science - Machine Learning},
  file = {/home/zohar/Zotero/storage/EE7PPVXP/Carlini et al. - 2021 - Extracting Training Data from Large Language Model.pdf;/home/zohar/Zotero/storage/Y7TCSX6P/2012.html}
}

@misc{chakrabortyAdversarialAttacksDefences2018,
  title = {Adversarial {{Attacks}} and {{Defences}}: {{A Survey}}},
  shorttitle = {Adversarial {{Attacks}} and {{Defences}}},
  author = {Chakraborty, Anirban and Alam, Manaar and Dey, Vishal and Chattopadhyay, Anupam and Mukhopadhyay, Debdeep},
  year = {2018},
  month = sep,
  number = {arXiv:1810.00069},
  eprint = {1810.00069},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  urldate = {2023-05-25},
  abstract = {Deep learning has emerged as a strong and efficient framework that can be applied to a broad spectrum of complex learning problems which were difficult to solve using the traditional machine learning techniques in the past. In the last few years, deep learning has advanced radically in such a way that it can surpass human-level performance on a number of tasks. As a consequence, deep learning is being extensively used in most of the recent day-to-day applications. However, security of deep learning systems are vulnerable to crafted adversarial examples, which may be imperceptible to the human eye, but can lead the model to misclassify the output. In recent times, different types of adversaries based on their threat model leverage these vulnerabilities to compromise a deep learning system where adversaries have high incentives. Hence, it is extremely important to provide robustness to deep learning algorithms against these adversaries. However, there are only a few strong countermeasures which can be used in all types of attack scenarios to design a robust deep learning system. In this paper, we attempt to provide a detailed discussion on different types of adversarial attacks with various threat models and also elaborate the efficiency and challenges of recent countermeasures against them.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/zohar/Zotero/storage/ZS67TLKN/Chakraborty et al. - 2018 - Adversarial Attacks and Defences A Survey.pdf}
}

@misc{compagnoDonSkypeType2017,
  title = {Don't {{Skype}} \& {{Type}}! {{Acoustic Eavesdropping}} in {{Voice-Over-IP}}},
  author = {Compagno, Alberto and Conti, Mauro and Lain, Daniele and Tsudik, Gene},
  year = {2017},
  month = mar,
  number = {arXiv:1609.09359},
  eprint = {1609.09359},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-04-26},
  abstract = {Acoustic emanations of computer keyboards represent a serious privacy issue. As demonstrated in prior work, physical properties of keystroke sounds might reveal what a user is typing. However, previous attacks assumed relatively strong adversary models that are not very practical in many realworld settings. Such strong models assume: (i) adversary's physical proximity to the victim, (ii) precise profiling of the victim's typing style and keyboard, and/or (iii) significant amount of victim's typed information (and its corresponding sounds) available to the adversary.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Cryptography and Security},
  note = {Comment: To appear in ACM Asia Conference on Computer and Communications Security (ASIACCS) 2017. 13 pages, 17 figures},
  file = {/home/zohar/Zotero/storage/XM99BKK5/Compagno et al. - 2017 - Don't Skype & Type! Acoustic Eavesdropping in Voic.pdf}
}

@inproceedings{fredriksonModelInversionAttacks2015,
  title = {Model {{Inversion Attacks}} That {{Exploit Confidence Information}} and {{Basic Countermeasures}}},
  booktitle = {Proceedings of the 22nd {{ACM SIGSAC Conference}} on {{Computer}} and {{Communications Security}}},
  author = {Fredrikson, Matt and Jha, Somesh and Ristenpart, Thomas},
  year = {2015},
  month = oct,
  series = {{{CCS}} '15},
  pages = {1322--1333},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2810103.2813677},
  urldate = {2023-05-25},
  abstract = {Machine-learning (ML) algorithms are increasingly utilized in privacy-sensitive applications such as predicting lifestyle choices, making medical diagnoses, and facial recognition. In a model inversion attack, recently introduced in a case study of linear classifiers in personalized medicine by Fredrikson et al., adversarial access to an ML model is abused to learn sensitive genomic information about individuals. Whether model inversion attacks apply to settings outside theirs, however, is unknown. We develop a new class of model inversion attack that exploits confidence values revealed along with predictions. Our new attacks are applicable in a variety of settings, and we explore two in depth: decision trees for lifestyle surveys as used on machine-learning-as-a-service systems and neural networks for facial recognition. In both cases confidence values are revealed to those with the ability to make prediction queries to models. We experimentally show attacks that are able to estimate whether a respondent in a lifestyle survey admitted to cheating on their significant other and, in the other context, show how to recover recognizable images of people's faces given only their name and access to the ML model. We also initiate experimental exploration of natural countermeasures, investigating a privacy-aware decision tree training algorithm that is a simple variant of CART learning, as well as revealing only rounded confidence values. The lesson that emerges is that one can avoid these kinds of MI attacks with negligible degradation to utility.},
  isbn = {978-1-4503-3832-5},
  keywords = {attacks,machine learning,privacy},
  file = {/home/zohar/Zotero/storage/6U2M8EJF/Fredrikson et al. - 2015 - Model Inversion Attacks that Exploit Confidence In.pdf}
}

@article{hatamizadehGradientInversionAttacks2023,
  title = {Do {{Gradient Inversion Attacks Make Federated Learning Unsafe}}?},
  author = {Hatamizadeh, Ali and Yin, Hongxu and Molchanov, Pavlo and Myronenko, Andriy and Li, Wenqi and Dogra, Prerna and Feng, Andrew and Flores, Mona G. and Kautz, Jan and Xu, Daguang and Roth, Holger R.},
  year = {2023},
  journal = {IEEE Transactions on Medical Imaging},
  pages = {1--1},
  issn = {1558-254X},
  doi = {10.1109/TMI.2023.3239391},
  abstract = {Federated learning (FL) allows the collaborative training of AI models without needing to share raw data. This capability makes it especially interesting for healthcare applications where patient and data privacy is of utmost concern. However, recent works on the inversion of deep neural networks from model gradients raised concerns about the security of FL in preventing the leakage of training data. In this work, we show that these attacks presented in the literature are impractical in FL use-cases where the clients' training involves updating the Batch Normalization (BN) statistics and provide a new baseline attack that works for such scenarios. Furthermore, we present new ways to measure and visualize potential data leakage in FL. Our work is a step towards establishing reproducible methods of measuring data leakage in FL and could help determine the optimal tradeoffs between privacy-preserving techniques, such as differential privacy, and model accuracy based on quantifiable metrics.},
  keywords = {Artificial intelligence,Computational modeling,Data models,Deep Learning,Federated Learning,Gradient Inversion,Image reconstruction,Medical services,Patient Privacy,Security,Servers,Training},
  file = {/home/zohar/Zotero/storage/95455RTE/Hatamizadeh et al. - 2023 - Do Gradient Inversion Attacks Make Federated Learn.pdf;/home/zohar/Zotero/storage/VQG7S3DC/10025466.html}
}

@misc{liPrivacyThreatsAnalysis2021,
  title = {Privacy {{Threats Analysis}} to {{Secure Federated Learning}}},
  author = {Li, Yuchen and Bao, Yifan and Xiang, Liyao and Liu, Junhan and Chen, Cen and Wang, Li and Wang, Xinbing},
  year = {2021},
  month = jun,
  number = {arXiv:2106.13076},
  eprint = {2106.13076},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-06-09},
  abstract = {Federated learning is emerging as a machine learning technique that trains a model across multiple decentralized parties. It is renowned for preserving privacy as the data never leaves the computational devices, and recent approaches further enhance its privacy by hiding messages transferred in encryption. However, we found that despite the efforts, federated learning remains privacy-threatening, due to its interactive nature across different parties. In this paper, we analyze the privacy threats in industrial-level federated learning frameworks with secure computation, and reveal such threats widely exist in typical machine learning models such as linear regression, logistic regression and decision tree. For the linear and logistic regression, we show through theoretical analysis that it is possible for the attacker to invert the entire private input of the victim, given very few information. For the decision tree model, we launch an attack to infer the range of victim's private inputs. All attacks are evaluated on popular federated learning frameworks and real-world datasets.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Cryptography and Security,Computer Science - Machine Learning},
  file = {/home/zohar/Zotero/storage/RWVTSZU6/Li et al. - 2021 - Privacy Threats Analysis to Secure Federated Learn.pdf}
}

@inproceedings{liResSFLResistanceTransfer2022,
  title = {{{ResSFL}}: {{A Resistance Transfer Framework}} for {{Defending Model Inversion Attack}} in {{Split Federated Learning}}},
  shorttitle = {{{ResSFL}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Li, Jingtao and Rakin, Adnan Siraj and Chen, Xing and He, Zhezhi and Fan, Deliang and Chakrabarti, Chaitali},
  year = {2022},
  pages = {10194--10202},
  urldate = {2023-06-19},
  langid = {english},
  file = {/home/zohar/Zotero/storage/IEKI6VGS/Li et al. - 2022 - ResSFL A Resistance Transfer Framework for Defend.pdf}
}

@inproceedings{papernotSoKSecurityPrivacy2018,
  title = {{{SoK}}: {{Security}} and {{Privacy}} in {{Machine Learning}}},
  shorttitle = {{{SoK}}},
  booktitle = {2018 {{IEEE European Symposium}} on {{Security}} and {{Privacy}} ({{EuroS}}\&{{P}})},
  author = {Papernot, Nicolas and McDaniel, Patrick and Sinha, Arunesh and Wellman, Michael P.},
  year = {2018},
  month = apr,
  pages = {399--414},
  doi = {10.1109/EuroSP.2018.00035},
  abstract = {Advances in machine learning (ML) in recent years have enabled a dizzying array of applications such as data analytics, autonomous systems, and security diagnostics. ML is now pervasive-new systems and models are being deployed in every domain imaginable, leading to widespread deployment of software based inference and decision making. There is growing recognition that ML exposes new vulnerabilities in software systems, yet the technical community's understanding of the nature and extent of these vulnerabilities remains limited. We systematize findings on ML security and privacy, focusing on attacks identified on these systems and defenses crafted to date.We articulate a comprehensive threat model for ML, and categorize attacks and defenses within an adversarial framework. Key insights resulting from works both in the ML and security communities are identified and the effectiveness of approaches are related to structural elements of ML algorithms and the data used to train them. In particular, it is apparent that constructing a theoretical understanding of the sensitivity of modern ML algorithms to the data they analyze, \`a la PAC theory, will foster a science of security and privacy in ML.},
  keywords = {Analytical models,Computational modeling,Data models,machine learning,Machine learning,privacy,Privacy,security,Security,Training},
  file = {/home/zohar/Zotero/storage/4GAWV99S/Papernot et al. - 2018 - SoK Security and Privacy in Machine Learning.pdf;/home/zohar/Zotero/storage/RXH42E4I/stamp.html}
}

@article{shinEmpiricalAnalysisImage2023,
  title = {An Empirical Analysis of Image Augmentation against Model Inversion Attack in Federated Learning},
  author = {Shin, Seunghyeon and Boyapati, Mallika and Suo, Kun and Kang, Kyungtae and Son, Junggab},
  year = {2023},
  month = feb,
  journal = {Cluster Computing},
  volume = {26},
  number = {1},
  pages = {349--366},
  issn = {1573-7543},
  doi = {10.1007/s10586-022-03596-1},
  urldate = {2023-06-19},
  abstract = {Federated Learning (FL) is a technology that facilitates a sophisticated way to train distributed data. As the FL does not expose sensitive data in the training process, it was considered privacy-safe deep learning. However, a few recent studies proved that it is possible to expose the hidden data by exploiting the shared models only. One common solution for the data exposure is differential privacy that adds noise to hinder such an attack, however, it inevitably involves a trade-off between privacy and utility. This paper demonstrates the effectiveness of image augmentation as an alternative defense strategy that has less impact of the trade-off. We conduct comprehensive experiments on the CIFAR-10 and CIFAR-100 datasets with 14 augmentations and 9 magnitudes. As a result, the best combination of augmentation and magnitude for each image class in the datasets was discovered. Also, our results show that a well-fitted augmentation strategy can outperform differential privacy.},
  langid = {english},
  keywords = {Defensive augmentation,Differential privacy,Federated learning,Image augmentation,Model inversion attack},
  file = {/home/zohar/Zotero/storage/S7F5GD7L/Shin et al. - 2023 - An empirical analysis of image augmentation agains.pdf}
}
